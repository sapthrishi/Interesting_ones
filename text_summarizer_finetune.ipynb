{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5892c569-c49a-44aa-9549-c4282afb07e1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Standard Libraries\n",
    "import re\n",
    "import pickle\n",
    "import mlflow\n",
    "\n",
    "# Data Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import swifter\n",
    "\n",
    "# Data Preprocessing\n",
    "import nltk\n",
    "from nltk.util import ngrams\n",
    "from cleantext import clean\n",
    "\n",
    "# Metrics\n",
    "from rouge import Rouge \n",
    "\n",
    "# Data Visualisation\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.core.display import display, HTML\n",
    "\n",
    "# To Call the API\n",
    "import requests\n",
    "import json\n",
    "\n",
    "# Functionality\n",
    "from typing import List, Dict, Union\n",
    "\n",
    "from datasets import load_dataset\n",
    "from datasets import Dataset\n",
    "\n",
    "import evaluate\n",
    "from transformers import DataCollatorForSeq2Seq\n",
    "from transformers import AutoModelForSeq2SeqLM, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "\n",
    "import huggingface_hub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "02332036-939e-4b5b-b35f-3477709cfa1c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## 1. Login to Huggingface API to save your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ed820b9b-30c4-44b1-843a-9300768cf157",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\nToken is valid.\nYour token has been saved to /root/.cache/huggingface/token\nLogin successful\n"
     ]
    }
   ],
   "source": [
    "huggingface_hub.login(token=\"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\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5ae75928-85ba-4a12-8873-6887a1aa1d47",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# 2. Curate data for fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dd057676-cc68-4e6a-8191-842e17c6a855",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.builder:Found cached dataset billsum (/root/.cache/huggingface/datasets/billsum/default/3.0.0/75cf1719d38d6553aa0e0714c393c74579b083ae6e164b2543684e3e92e0c4cc)\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "328913d2ecff4da29595ab101f72401c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[7]: DatasetDict({\n    train: Dataset({\n        features: ['text', 'summary', 'title'],\n        num_rows: 18949\n    })\n    test: Dataset({\n        features: ['text', 'summary', 'title'],\n        num_rows: 3269\n    })\n    ca_test: Dataset({\n        features: ['text', 'summary', 'title'],\n        num_rows: 1237\n    })\n})"
     ]
    }
   ],
   "source": [
    "dataset_name   = \"billsum\"\n",
    "dataset = load_dataset(dataset_name)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "85e7fa28-8043-43d5-b9f5-478bb12703a9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[8]: {'text': \"SECTION 1. LIABILITY OF BUSINESS ENTITIES PROVIDING USE OF FACILITIES \\n              TO NONPROFIT ORGANIZATIONS.\\n\\n    (a) Definitions.--In this section:\\n            (1) Business entity.--The term ``business entity'' means a \\n        firm, corporation, association, partnership, consortium, joint \\n        venture, or other form of enterprise.\\n            (2) Facility.--The term ``facility'' means any real \\n        property, including any building, improvement, or appurtenance.\\n            (3) Gross negligence.--The term ``gross negligence'' means \\n        voluntary and conscious conduct by a person with knowledge (at \\n        the time of the conduct) that the conduct is likely to be \\n        harmful to the health or well-being of another person.\\n            (4) Intentional misconduct.--The term ``intentional \\n        misconduct'' means conduct by a person with knowledge (at the \\n        time of the conduct) that the conduct is harmful to the health \\n        or well-being of another person.\\n            (5) Nonprofit organization.--The term ``nonprofit \\n        organization'' means--\\n                    (A) any organization described in section 501(c)(3) \\n                of the Internal Revenue Code of 1986 and exempt from \\n                tax under section 501(a) of such Code; or\\n                    (B) any not-for-profit organization organized and \\n                conducted for public benefit and operated primarily for \\n                charitable, civic, educational, religious, welfare, or \\n                health purposes.\\n            (6) State.--The term ``State'' means each of the several \\n        States, the District of Columbia, the Commonwealth of Puerto \\n        Rico, the Virgin Islands, Guam, American Samoa, the Northern \\n        Mariana Islands, any other territory or possession of the \\n        United States, or any political subdivision of any such State, \\n        territory, or possession.\\n    (b) Limitation on Liability.--\\n            (1) In general.--Subject to subsection (c), a business \\n        entity shall not be subject to civil liability relating to any \\n        injury or death occurring at a facility of the business entity \\n        in connection with a use of such facility by a nonprofit \\n        organization if--\\n                    (A) the use occurs outside of the scope of business \\n                of the business entity;\\n                    (B) such injury or death occurs during a period \\n                that such facility is used by the nonprofit \\n                organization; and\\n                    (C) the business entity authorized the use of such \\n                facility by the nonprofit organization.\\n            (2) Application.--This subsection shall apply--\\n                    (A) with respect to civil liability under Federal \\n                and State law; and\\n                    (B) regardless of whether a nonprofit organization \\n                pays for the use of a facility.\\n    (c) Exception for Liability.--Subsection (b) shall not apply to an \\ninjury or death that results from an act or omission of a business \\nentity that constitutes gross negligence or intentional misconduct, \\nincluding any misconduct that--\\n            (1) constitutes a crime of violence (as that term is \\n        defined in section 16 of title 18, United States Code) or act \\n        of international terrorism (as that term is defined in section \\n        2331 of title 18) for which the defendant has been convicted in \\n        any court;\\n            (2) constitutes a hate crime (as that term is used in the \\n        Hate Crime Statistics Act (28 U.S.C. 534 note));\\n            (3) involves a sexual offense, as defined by applicable \\n        State law, for which the defendant has been convicted in any \\n        court; or\\n            (4) involves misconduct for which the defendant has been \\n        found to have violated a Federal or State civil rights law.\\n    (d) Superseding Provision.--\\n            (1) In general.--Subject to paragraph (2) and subsection \\n        (e), this Act preempts the laws of any State to the extent that \\n        such laws are inconsistent with this Act, except that this Act \\n        shall not preempt any State law that provides additional \\n        protection from liability for a business entity for an injury \\n        or death with respect to which conditions under subparagraphs \\n        (A) through (C) of subsection (b)(1) apply.\\n            (2) Limitation.--Nothing in this Act shall be construed to \\n        supersede any Federal or State health or safety law.\\n    (e) Election of State Regarding Nonapplicability.--This Act shall \\nnot apply to any civil action in a State court against a business \\nentity in which all parties are citizens of the State if such State \\nenacts a statute--\\n            (1) citing the authority of this subsection;\\n            (2) declaring the election of such State that this Act \\n        shall not apply to such civil action in the State; and\\n            (3) containing no other provision.\",\n 'summary': \"Shields a business entity from civil liability relating to any injury or death occurring at a facility of that entity in connection with a use of such facility by a nonprofit organization if: (1) the use occurs outside the scope of business of the business entity; (2) such injury or death occurs during a period that such facility is used by such organization; and (3) the business entity authorized the use of such facility by the organization. \\nMakes this Act inapplicable to an injury or death that results from an act or omission of a business entity that constitutes gross negligence or intentional misconduct, including misconduct that: (1) constitutes a hate crime or a crime of violence or act of international terrorism for which the defendant has been convicted in any court; or (2) involves a sexual offense for which the defendant has been convicted in any court or misconduct for which the defendant has been found to have violated a Federal or State civil rights law. \\nPreempts State laws to the extent that such laws are inconsistent with this Act, except State law that provides additional protection from liability.  Specifies that this Act shall not be construed to supersede any Federal or State health or safety law. \\nMakes this Act inapplicable to any civil action in a State court against a business entity in which all parties are citizens of the State if such State, citing this Act's authority and containing no other provision, enacts a statute declaring the State's election that this Act shall not apply to such action in the State.\",\n 'title': 'A bill to limit the civil liability of business entities providing use of facilities to nonprofit organizations.'}"
     ]
    }
   ],
   "source": [
    "dataset[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8a845a71-1ee8-45c6-a337-584271016a17",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18949, 2)\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SECTION 1. LIABILITY OF BUSINESS ENTITIES PROV...</td>\n",
       "      <td>Shields a business entity from civil liability...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SECTION 1. SHORT TITLE.\\n\\n    This Act may be...</td>\n",
       "      <td>Human Rights Information Act - Requires certai...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SECTION 1. SHORT TITLE.\\n\\n    This Act may be...</td>\n",
       "      <td>Jackie Robinson Commemorative Coin Act - Direc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SECTION 1. NONRECOGNITION OF GAIN WHERE ROLLOV...</td>\n",
       "      <td>Amends the Internal Revenue Code to provide (t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SECTION 1. SHORT TITLE.\\n\\n    This Act may be...</td>\n",
       "      <td>Native American Energy Act - (Sec. 3) Amends t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>summary</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>SECTION 1. LIABILITY OF BUSINESS ENTITIES PROV...</td>\n      <td>Shields a business entity from civil liability...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>SECTION 1. SHORT TITLE.\\n\\n    This Act may be...</td>\n      <td>Human Rights Information Act - Requires certai...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>SECTION 1. SHORT TITLE.\\n\\n    This Act may be...</td>\n      <td>Jackie Robinson Commemorative Coin Act - Direc...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>SECTION 1. NONRECOGNITION OF GAIN WHERE ROLLOV...</td>\n      <td>Amends the Internal Revenue Code to provide (t...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>SECTION 1. SHORT TITLE.\\n\\n    This Act may be...</td>\n      <td>Native American Energy Act - (Sec. 3) Amends t...</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "textData": null,
       "type": "htmlSandbox"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# to pandas df\n",
    "data_train = pd.DataFrame(dataset['train'])[[\"text\", \"summary\"]]   #.sample(n=32, random_state=1)\n",
    "data_val   = pd.DataFrame(dataset['test'])[[\"text\", \"summary\"]]    #.sample(n=32, random_state=1)\n",
    "data_test  = pd.DataFrame(dataset['ca_test'])[[\"text\", \"summary\"]] #.sample(n=32, random_state=1)\n",
    "\n",
    "print(data_train.shape)\n",
    "data_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4abe5100-94fb-4427-8756-b1568bca67f6",
     "showTitle": true,
     "title": "Data Pre-processing: Clean text"
    }
   },
   "outputs": [],
   "source": [
    "def replacer(match):\n",
    "  \"\"\" If there is a match - we add a blank space to the right or to the left of the match.\n",
    "\n",
    "  Example usage:\n",
    "    match = <re.Match object; span=(0, 1), match='('>\n",
    "    return: '( '\n",
    "\n",
    "    match = <re.Match object; span=(0, 1), match=')'>\n",
    "    return: ' )'\n",
    "  \"\"\"\n",
    "\n",
    "  if match.group(1) is not None:\n",
    "    return '{} '.format(match.group(1))\n",
    "  return ' {}'.format(match.group(2))\n",
    "\n",
    "def clean_text(txt:str)->str:\n",
    "  \"\"\" Takes text as input, matches punctuation and spaces it out from the word.\n",
    "\n",
    "  Depending on the format of the punctuation - if there is a space before it or after it, \n",
    "  the original string is transformed. In the string 'March 18 (Reuters) - Hotel revenue'\n",
    "  we see that there is a space before the first bracket '18 (Reuters', therefore a space \n",
    "  will be added after the bracket as well '18 ( reuters'. \n",
    "\n",
    "  txt: String text. \n",
    "\n",
    "  Example usage:\n",
    "    txt = 'March 18 (Reuters) - Hotel revenue slumped globally in February'\n",
    "    return: 'march 18 ( reuters ) -  hotel revenue slumped globally in february'\n",
    "\n",
    "  return: Lowercased, cleaned text with spaced out punctuation.\n",
    "  \"\"\"\n",
    "\n",
    "  rx = re.compile(r'^(\\W+)|(\\W+)$')\n",
    "  return clean(\" \".join([rx.sub(replacer, word) for word in txt.split()]).lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4f5d475a-676f-4a9f-922e-545e181d6487",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c3b64082b4d48dba06b7aa961a87c39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/18949 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "281e174db1ec4f78b264a699cae1aa5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/18949 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e4852b36f5b4f698889a518314dfbb4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad0402351de144868bc1b0bc1629e3ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63458e0b16d94c4eb7bc8752b9901260",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fda74730f5d142798f3dd4544b5ee4a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_train['text_processed']    = data_train.text.swifter.apply(clean_text)\n",
    "data_train['summary_processed'] = data_train.summary.swifter.apply(clean_text)\n",
    "data_train['text']              = data_train['text_processed']\n",
    "data_train['summary']           = data_train['summary_processed']\n",
    "data_train                      = data_train[[\"text\", \"summary\"]]\n",
    "\n",
    "data_test['text_processed']     = data_test.text.swifter.apply(clean_text)\n",
    "data_test['summary_processed']  = data_test.summary.swifter.apply(clean_text)\n",
    "data_test['text']               = data_test['text_processed']\n",
    "data_test['summary']            = data_test['summary_processed']\n",
    "data_test                       = data_test[[\"text\", \"summary\"]]\n",
    "\n",
    "data_val['text_processed']      = data_val.text.swifter.apply(clean_text)\n",
    "data_val['summary_processed']   = data_val.summary.swifter.apply(clean_text)\n",
    "data_val['text']                = data_val['text_processed']\n",
    "data_val['summary']             = data_val['summary_processed']\n",
    "data_val                        = data_val[[\"text\", \"summary\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7af48215-d322-49dc-bad0-5f45feaf1240",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data_val   = Dataset.from_pandas(data_val)\n",
    "data_test  = Dataset.from_pandas(data_test)\n",
    "data_train = Dataset.from_pandas(data_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c697d2df-637e-469e-b99e-c2ff487d7fa9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[13]: \"shields a business entity from civil liability relating to any injury or death occurring at a facility of that entity in connection with a use of such facility by a nonprofit organization if : ( 1 ) the use occurs outside the scope of business of the business entity ; ( 2 ) such injury or death occurs during a period that such facility is used by such organization ; and ( 3 ) the business entity authorized the use of such facility by the organization . makes this act inapplicable to an injury or death that results from an act or omission of a business entity that constitutes gross negligence or intentional misconduct , including misconduct that : ( 1 ) constitutes a hate crime or a crime of violence or act of international terrorism for which the defendant has been convicted in any court ; or ( 2 ) involves a sexual offense for which the defendant has been convicted in any court or misconduct for which the defendant has been found to have violated a federal or state civil rights law . preempts state laws to the extent that such laws are inconsistent with this act , except state law that provides additional protection from liability . specifies that this act shall not be construed to supersede any federal or state health or safety law . makes this act inapplicable to any civil action in a state court against a business entity in which all parties are citizens of the state if such state , citing this act's authority and containing no other provision , enacts a statute declaring the state's election that this act shall not apply to such action in the state .\""
     ]
    }
   ],
   "source": [
    "data_train['summary'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0841e82b-cb92-48b2-991d-08dc53628be7",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# 2. Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "54144747-c386-49b0-8c3f-2a84d8707b72",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### 2.1 Tokenize: words to tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4b16ef06-7039-4b01-8a84-4ec79633e276",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_name = \"facebook/bart-large-cnn\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4c766d22-5045-4966-9451-20b5836a98c3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[15]: {'input_ids': [0, 100, 2638, 2600, 5, 27689, 3100, 328, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1]}"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer(text = \"I loved reading the Hunger Games!\")\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5a0e5d58-b9e5-499d-91aa-49db2f067cb6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[17]: ['<s>', 'I', 'Ġloved', 'Ġreading', 'Ġthe', 'ĠHunger', 'ĠGames', '!', '</s>']"
     ]
    }
   ],
   "source": [
    "tokenizer.convert_ids_to_tokens(inputs.input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7226b0e5-dc5e-4ae9-a828-beb4e8b8a47d",
     "showTitle": true,
     "title": "Convert words to tokens"
    }
   },
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "\n",
    "    inputs = [doc for doc in examples[\"text\"]]\n",
    "    model_inputs = tokenizer(inputs, max_length=1024, truncation=True)\n",
    "    labels = tokenizer(examples[\"summary\"], max_length=128, truncation=True)\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d37bce2a-b175-47ec-b29d-573b2f5e70df",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "701abfc6e2a94d5c928aede14917b063",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/18949 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b494c62e63d41a0bf14939cdb290c41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/32 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4eb2a0f81a5b458fb9f2677490498752",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/32 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[19]: Dataset({\n    features: ['text', 'summary', 'input_ids', 'attention_mask', 'labels'],\n    num_rows: 18949\n})"
     ]
    }
   ],
   "source": [
    "data_train = data_train.map(preprocess_function, batched=True)\n",
    "data_test  = data_test.map(preprocess_function, batched=True)\n",
    "data_val   = data_val.map(preprocess_function, batched=True)\n",
    "\n",
    "data_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f63567d8-a80c-44d7-a846-749e54d2ee6b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### 2.2 Define evaluation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c2d163f5-6fed-4f67-aaa5-badc54234554",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "rouge = evaluate.load(\"rouge\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    result = rouge.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
    "\n",
    "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in predictions]\n",
    "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "\n",
    "    return {k: round(v, 4) for k, v in result.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "41cf31da-893b-4311-961b-c4660347ceb5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### 2.3 Define model and data-collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "19ce3048-16be-419e-a404-afce809a8830",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "448a6f50-a4de-42a9-9d8d-d2765f18dcd0",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### 2.4 Call model train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b579de3b-26dd-4eb2-bf0b-f1ceb1c582ed",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "model_name = \"bart-large-cnn-billsum\"\n",
    "mlflow.pytorch.autolog(\n",
    "    log_input_examples=False,\n",
    "    log_model_signatures=False,\n",
    "    log_models=False,\n",
    "    disable=True,\n",
    "    exclusive=True,\n",
    "    disable_for_unsupported_versions=True,\n",
    "    silent=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f7b187a9-ff48-4149-a0c4-b9c09c7f6766",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda_amp half precision backend\nThe following columns in the training set don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: __index_level_0__, summary, text. If __index_level_0__, summary, text are not expected by `BartForConditionalGeneration.forward`,  you can safely ignore this message.\n***** Running training *****\n  Num examples = 32\n  Num Epochs = 4\n  Instantaneous batch size per device = 2\n  Total train batch size (w. parallel, distributed & accumulation) = 2\n  Gradient Accumulation steps = 1\n  Total optimization steps = 64\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='64' max='64' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [64/64 03:46, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rouge1</th>\n",
       "      <th>Rouge2</th>\n",
       "      <th>Rougel</th>\n",
       "      <th>Rougelsum</th>\n",
       "      <th>Gen Len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.763327</td>\n",
       "      <td>0.500300</td>\n",
       "      <td>0.239300</td>\n",
       "      <td>0.301400</td>\n",
       "      <td>0.302600</td>\n",
       "      <td>140.656200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.308880</td>\n",
       "      <td>0.541500</td>\n",
       "      <td>0.275800</td>\n",
       "      <td>0.329100</td>\n",
       "      <td>0.330500</td>\n",
       "      <td>141.781200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.046858</td>\n",
       "      <td>0.559600</td>\n",
       "      <td>0.303100</td>\n",
       "      <td>0.337900</td>\n",
       "      <td>0.337600</td>\n",
       "      <td>139.875000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.907518</td>\n",
       "      <td>0.572500</td>\n",
       "      <td>0.312800</td>\n",
       "      <td>0.350200</td>\n",
       "      <td>0.349200</td>\n",
       "      <td>141.343800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "\n    <div>\n      \n      <progress value='64' max='64' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [64/64 03:46, Epoch 4/4]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Rouge1</th>\n      <th>Rouge2</th>\n      <th>Rougel</th>\n      <th>Rougelsum</th>\n      <th>Gen Len</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>No log</td>\n      <td>1.763327</td>\n      <td>0.500300</td>\n      <td>0.239300</td>\n      <td>0.301400</td>\n      <td>0.302600</td>\n      <td>140.656200</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>No log</td>\n      <td>1.308880</td>\n      <td>0.541500</td>\n      <td>0.275800</td>\n      <td>0.329100</td>\n      <td>0.330500</td>\n      <td>141.781200</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>No log</td>\n      <td>1.046858</td>\n      <td>0.559600</td>\n      <td>0.303100</td>\n      <td>0.337900</td>\n      <td>0.337600</td>\n      <td>139.875000</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>No log</td>\n      <td>0.907518</td>\n      <td>0.572500</td>\n      <td>0.312800</td>\n      <td>0.350200</td>\n      <td>0.349200</td>\n      <td>141.343800</td>\n    </tr>\n  </tbody>\n</table><p>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "textData": null,
       "type": "htmlSandbox"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: __index_level_0__, summary, text. If __index_level_0__, summary, text are not expected by `BartForConditionalGeneration.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 32\n  Batch size = 2\nThe following columns in the evaluation set don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: __index_level_0__, summary, text. If __index_level_0__, summary, text are not expected by `BartForConditionalGeneration.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 32\n  Batch size = 2\nThe following columns in the evaluation set don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: __index_level_0__, summary, text. If __index_level_0__, summary, text are not expected by `BartForConditionalGeneration.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 32\n  Batch size = 2\nThe following columns in the evaluation set don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: __index_level_0__, summary, text. If __index_level_0__, summary, text are not expected by `BartForConditionalGeneration.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 32\n  Batch size = 2\n\n\nTraining completed. Do not forget to share your model on huggingface.co/models =)\n\n\nOut[23]: TrainOutput(global_step=64, training_loss=1.7066502571105957, metrics={'train_runtime': 228.6014, 'train_samples_per_second': 0.56, 'train_steps_per_second': 0.28, 'total_flos': 277389389070336.0, 'train_loss': 1.7066502571105957, 'epoch': 4.0})"
     ]
    }
   ],
   "source": [
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=model_name,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=3,\n",
    "    num_train_epochs=4,\n",
    "    predict_with_generate=True,\n",
    "    fp16=True,\n",
    "    push_to_hub=False,\n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=data_train,\n",
    "    eval_dataset=data_val,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7be2d90a-62ea-421a-bd6a-e9cff2a51185",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# 3. Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f9d83b8e-2519-4e22-af15-82706497be71",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# trainer.push_to_hub(model_name, use_auth_token=\"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\")\n",
    "# model.push_to_hub(model_name, use_temp_dir=True)\n",
    "# tokenizer.push_to_hub(model_name, use_temp_dir=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c5637f7c-3fb8-4abf-8bdb-af25c762b79d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### 3.1 Save model to local disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d111106b-5e7d-4ecc-8411-ca35e474df1d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration saved in saved_bart-large-cnn-billsum/config.json\nModel weights saved in saved_bart-large-cnn-billsum/pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "model.save_pretrained(f\"saved_{model_name}\", repo_name=model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "54f667c1-6496-40c4-a10f-0606c7ac4fdb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizer config file saved in saved_bart-large-cnn-billsum/tokenizer_config.json\nSpecial tokens file saved in saved_bart-large-cnn-billsum/special_tokens_map.json\nOut[25]: ('saved_bart-large-cnn-billsum/tokenizer_config.json',\n 'saved_bart-large-cnn-billsum/special_tokens_map.json',\n 'saved_bart-large-cnn-billsum/vocab.json',\n 'saved_bart-large-cnn-billsum/merges.txt',\n 'saved_bart-large-cnn-billsum/added_tokens.json',\n 'saved_bart-large-cnn-billsum/tokenizer.json')"
     ]
    }
   ],
   "source": [
    "tokenizer_files_to_save = tokenizer.save_pretrained(f\"saved_{model_name}\", repo_name=model_name)\n",
    "tokenizer_files_to_save"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d95bfbd0-26e4-481c-bdea-b659e72c0d02",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### 3.2 install git-lfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7594d695-567d-4106-b4c8-aa4d14a29980",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\rReading package lists... 0%\r\rReading package lists... 0%\r\rReading package lists... 0%\r\rReading package lists... 4%\r\rReading package lists... 4%\r\rReading package lists... 5%\r\rReading package lists... 5%\r\rReading package lists... 48%\r\rReading package lists... 48%\r\rReading package lists... 48%\r\rReading package lists... 48%\r\rReading package lists... 60%\r\rReading package lists... 60%\r\rReading package lists... 63%\r\rReading package lists... 69%\r\rReading package lists... 69%\r\rReading package lists... 74%\r\rReading package lists... 74%\r\rReading package lists... 74%\r\rReading package lists... 74%\r\rReading package lists... 74%\r\rReading package lists... 74%\r\rReading package lists... 74%\r\rReading package lists... 74%\r\rReading package lists... 84%\r\rReading package lists... 84%\r\rReading package lists... 92%\r\rReading package lists... 92%\r\rReading package lists... 96%\r\rReading package lists... 96%\r\rReading package lists... 96%\r\rReading package lists... 96%\r\rReading package lists... 98%\r\rReading package lists... 98%\r\rReading package lists... 99%\r\rReading package lists... 99%\r\rReading package lists... Done\r\r\n\rBuilding dependency tree... 0%\r\rBuilding dependency tree... 0%\r\rBuilding dependency tree... 50%\r\rBuilding dependency tree... 50%\r\rBuilding dependency tree       \r\r\n\rReading state information... 0%\r\rReading state information... 0%\r\rReading state information... Done\r\r\ngit-lfs is already the newest version (2.9.2-1).\r\n0 upgraded, 0 newly installed, 0 to remove and 24 not upgraded.\r\nGit LFS initialized.\r\n"
     ]
    }
   ],
   "source": [
    "!sudo apt-get install git-lfs\n",
    "!git lfs install --skip-repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "50f80188-d3f7-4798-a0f6-ff5a77f70fb6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import HfApi\n",
    "api = HfApi()\n",
    "api.create_repo(repo_id=model_name)\n",
    "\n",
    "# RepoUrl('https://huggingface.co/sapthrishi007/bart-large-cnn-billsum', endpoint='https://huggingface.co', repo_type='model', repo_id='sapthrishi007/bart-large-cnn-billsum')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d2858195-d190-4641-81fa-2161c40d2419",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### 3.3 Upload to Huggingface model hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "402b7d2c-c987-46d5-b00d-fab6bd6c521e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[29]: 'https://huggingface.co/sapthrishi007/bart-large-cnn-billsum/blob/main/config.json'"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import HfApi\n",
    "api = HfApi()\n",
    "\n",
    "api.upload_file(\n",
    "    path_or_fileobj=f\"saved_{model_name}/config.json\",\n",
    "    path_in_repo=\"config.json\",\n",
    "    repo_id=f\"sapthrishi007/{model_name}\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b3acecf3-dde8-4883-8121-365c94f47f4a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[30]: 'https://huggingface.co/sapthrishi007/bart-large-cnn-billsum/blob/main/pytorch_model.bin'"
     ]
    }
   ],
   "source": [
    "api.upload_file(\n",
    "    path_or_fileobj=f\"saved_{model_name}/pytorch_model.bin\",\n",
    "    path_in_repo=\"pytorch_model.bin\",\n",
    "    repo_id=f\"sapthrishi007/{model_name}\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "de2de747-5437-4a94-ab93-63eca88153ce",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "for tok_file in tokenizer_files_to_save:\n",
    "    if \"added_tokens.json\" in tok_file:\n",
    "        continue\n",
    "    \n",
    "    api.upload_file(\n",
    "        path_or_fileobj = tok_file,\n",
    "        path_in_repo    = tok_file.split(\"/\")[1],\n",
    "        repo_id         = f\"sapthrishi007/{model_name}\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "782db597-a56f-4851-b390-21903dcd250e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# 4. Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "90d16f20-4ae9-4a81-a489-d98285967de6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/sapthrishi007/bart-large-cnn-billsum/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/8038f16b1cca45c2e7b3b007ea262551609ce0dafa4baebf2588b54453a92892.f3e34561a5941e2e7d8f7bd3b39329501c6c5ebab51e82a90707cde99be7d3a6\nModel config BartConfig {\n  \"_name_or_path\": \"sapthrishi007/bart-large-cnn-billsum\",\n  \"_num_labels\": 3,\n  \"activation_dropout\": 0.0,\n  \"activation_function\": \"gelu\",\n  \"add_final_layer_norm\": false,\n  \"architectures\": [\n    \"BartForConditionalGeneration\"\n  ],\n  \"attention_dropout\": 0.0,\n  \"bos_token_id\": 0,\n  \"classif_dropout\": 0.0,\n  \"classifier_dropout\": 0.0,\n  \"d_model\": 1024,\n  \"decoder_attention_heads\": 16,\n  \"decoder_ffn_dim\": 4096,\n  \"decoder_layerdrop\": 0.0,\n  \"decoder_layers\": 12,\n  \"decoder_start_token_id\": 2,\n  \"dropout\": 0.1,\n  \"early_stopping\": true,\n  \"encoder_attention_heads\": 16,\n  \"encoder_ffn_dim\": 4096,\n  \"encoder_layerdrop\": 0.0,\n  \"encoder_layers\": 12,\n  \"eos_token_id\": 2,\n  \"force_bos_token_to_be_generated\": true,\n  \"forced_bos_token_id\": 0,\n  \"forced_eos_token_id\": 2,\n  \"gradient_checkpointing\": false,\n  \"id2label\": {\n    \"0\": \"LABEL_0\",\n    \"1\": \"LABEL_1\",\n    \"2\": \"LABEL_2\"\n  },\n  \"init_std\": 0.02,\n  \"is_encoder_decoder\": true,\n  \"label2id\": {\n    \"LABEL_0\": 0,\n    \"LABEL_1\": 1,\n    \"LABEL_2\": 2\n  },\n  \"length_penalty\": 2.0,\n  \"max_length\": 142,\n  \"max_position_embeddings\": 1024,\n  \"min_length\": 56,\n  \"model_type\": \"bart\",\n  \"no_repeat_ngram_size\": 3,\n  \"normalize_before\": false,\n  \"num_beams\": 4,\n  \"num_hidden_layers\": 12,\n  \"output_past\": true,\n  \"pad_token_id\": 1,\n  \"prefix\": \" \",\n  \"scale_embedding\": false,\n  \"task_specific_params\": {\n    \"summarization\": {\n      \"early_stopping\": true,\n      \"length_penalty\": 2.0,\n      \"max_length\": 142,\n      \"min_length\": 56,\n      \"no_repeat_ngram_size\": 3,\n      \"num_beams\": 4\n    }\n  },\n  \"torch_dtype\": \"float32\",\n  \"transformers_version\": \"4.21.2\",\n  \"use_cache\": true,\n  \"vocab_size\": 50264\n}\n\nloading configuration file https://huggingface.co/sapthrishi007/bart-large-cnn-billsum/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/8038f16b1cca45c2e7b3b007ea262551609ce0dafa4baebf2588b54453a92892.f3e34561a5941e2e7d8f7bd3b39329501c6c5ebab51e82a90707cde99be7d3a6\nModel config BartConfig {\n  \"_name_or_path\": \"sapthrishi007/bart-large-cnn-billsum\",\n  \"_num_labels\": 3,\n  \"activation_dropout\": 0.0,\n  \"activation_function\": \"gelu\",\n  \"add_final_layer_norm\": false,\n  \"architectures\": [\n    \"BartForConditionalGeneration\"\n  ],\n  \"attention_dropout\": 0.0,\n  \"bos_token_id\": 0,\n  \"classif_dropout\": 0.0,\n  \"classifier_dropout\": 0.0,\n  \"d_model\": 1024,\n  \"decoder_attention_heads\": 16,\n  \"decoder_ffn_dim\": 4096,\n  \"decoder_layerdrop\": 0.0,\n  \"decoder_layers\": 12,\n  \"decoder_start_token_id\": 2,\n  \"dropout\": 0.1,\n  \"early_stopping\": true,\n  \"encoder_attention_heads\": 16,\n  \"encoder_ffn_dim\": 4096,\n  \"encoder_layerdrop\": 0.0,\n  \"encoder_layers\": 12,\n  \"eos_token_id\": 2,\n  \"force_bos_token_to_be_generated\": true,\n  \"forced_bos_token_id\": 0,\n  \"forced_eos_token_id\": 2,\n  \"gradient_checkpointing\": false,\n  \"id2label\": {\n    \"0\": \"LABEL_0\",\n    \"1\": \"LABEL_1\",\n    \"2\": \"LABEL_2\"\n  },\n  \"init_std\": 0.02,\n  \"is_encoder_decoder\": true,\n  \"label2id\": {\n    \"LABEL_0\": 0,\n    \"LABEL_1\": 1,\n    \"LABEL_2\": 2\n  },\n  \"length_penalty\": 2.0,\n  \"max_length\": 142,\n  \"max_position_embeddings\": 1024,\n  \"min_length\": 56,\n  \"model_type\": \"bart\",\n  \"no_repeat_ngram_size\": 3,\n  \"normalize_before\": false,\n  \"num_beams\": 4,\n  \"num_hidden_layers\": 12,\n  \"output_past\": true,\n  \"pad_token_id\": 1,\n  \"prefix\": \" \",\n  \"scale_embedding\": false,\n  \"task_specific_params\": {\n    \"summarization\": {\n      \"early_stopping\": true,\n      \"length_penalty\": 2.0,\n      \"max_length\": 142,\n      \"min_length\": 56,\n      \"no_repeat_ngram_size\": 3,\n      \"num_beams\": 4\n    }\n  },\n  \"torch_dtype\": \"float32\",\n  \"transformers_version\": \"4.21.2\",\n  \"use_cache\": true,\n  \"vocab_size\": 50264\n}\n\nloading weights file https://huggingface.co/sapthrishi007/bart-large-cnn-billsum/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/78892576f07cbc355d434ccf6821bb1aa29902456b40a3969330f0805bc6af94.f441a44fc7388e2fc5745addb768cfc88a71655051134c2b8e099de0bccf4fc4\nAll model checkpoint weights were used when initializing BartForConditionalGeneration.\n\nAll the weights of BartForConditionalGeneration were initialized from the model checkpoint at sapthrishi007/bart-large-cnn-billsum.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use BartForConditionalGeneration for predictions without further training.\nloading file https://huggingface.co/sapthrishi007/bart-large-cnn-billsum/resolve/main/vocab.json from cache at /root/.cache/huggingface/transformers/6adc3e2dbd898f8b81ad752f7c2065cdaaf476611d192b307ca4ae1af8c20b9a.bfdcc444ff249bca1a95ca170ec350b442f81804d7df3a95a2252217574121d7\nloading file https://huggingface.co/sapthrishi007/bart-large-cnn-billsum/resolve/main/merges.txt from cache at /root/.cache/huggingface/transformers/3767cc74630efc82b7c56b21c0b5a2b5ba738f329d6b1b212511fa719943d75c.f5b91da9e34259b8f4d88dbc97c740667a0e8430b96314460cdb04e86d4fc435\nloading file https://huggingface.co/sapthrishi007/bart-large-cnn-billsum/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/734b42a353aaa1666ac85e57d18f037885a0324296649046f7425314131daf8b.e2f4eedc407009ae71961202385942a093ae25045c2f8223c20d64be0e0c9626\nloading file https://huggingface.co/sapthrishi007/bart-large-cnn-billsum/resolve/main/added_tokens.json from cache at None\nloading file https://huggingface.co/sapthrishi007/bart-large-cnn-billsum/resolve/main/special_tokens_map.json from cache at /root/.cache/huggingface/transformers/eedc5c65d9c4c7eb483611b492568267d297168dddd809ecb98d02cb10eb08af.50c9a6a3342271e7e900bb03520d7f844b78e2b2ef8352a0239b688c7d12bdc6\nloading file https://huggingface.co/sapthrishi007/bart-large-cnn-billsum/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/c2e0bb34e11343816256e7023867bf77f310e202a58eef8adb7579722f6e358a.09d320641b0a11e7f1d377a011e66b146deba95720abbb845658af5ce20d0caa\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "model_name   = \"sapthrishi007/bart-large-cnn-billsum\"\n",
    "\n",
    "summarizer   = pipeline(\"summarization\", model=model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3ef0b5a8-af34-4139-b65c-b3d0ddc05246",
     "showTitle": true,
     "title": "Teams meeting transcript"
    }
   },
   "outputs": [],
   "source": [
    "input_text_1 = \"\"\"Orchestration section right won't be decided for the example. Note we'll keep there is a there is currently a readme and then like feature notebook and model notebook.  Uh validations plus score, right? So we have these notebooks. And then  Actually, there are two different separate notebooks. So in the when we when we deploy this in an MLA environment, right? But we we decided that the way we will make users run these notebooks is through an orchestration job, right?  Uh-huh.  Like a like a job or a A just wanted wanted to know the plan. Are we? Are we achieving that in this 1/4 we are not? No, no. OK. So we, the community, the conversation I had with John and Lakshmi yesterday, that this quarter if we can just push this notebooks out there and if users are able to use it by are you reading going through the README file, create the silver table and then, you know, set up their own jobs. Yeah. Uh, then that is what we would do instead of like, this can be. And once we learn from that, then maybe in the next quarter we can optimize this process to have job orchestration in databricks. Maybe we can bring data pipeline or we might have to change data pipeline little but like we can do that as well. To. So that can be the future quarter effort because it will be right now if we do a job, OK, station, it would be still be like users creating their own silver table and then running the job orchestration to run the feature model build and model pipeline makes sense. To predict what we were. Yeah.  What we were thinking is in our orchestration section in the README notebook, we'll actually give them instructions on how to create a job and use a workflow just like how our data scientists are doing. Right. OK. They they create that workflow and we can have an example workflow already created in the environment where they can go see that workflow and then they create they can create themselves. OK. They. Yeah. Well, they won't see the workflow because we would demo it right in that integration. Yeah, read demo it. We demo it. We we would actually like in that room, but they're doing a video recording like we've done for other MLAs we would show them how to build to Lakshmi's point how to build a workflow which they could build on their own. Because what the the reality is, is that that there's a there's a notebook that happens in between feature, right. But prior to feature which is gonna be them creating the data sets in the format that we need, right? No. So they're basically creating the silver tables that we require and then we we don't know what that's at. We don't know what that's gonna be like cause they can onboard first party data in whatever format it's possible, right?\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3af64139-f5a7-441d-9d63-5c18d87c68d2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[34]: [{'summary_text': 'There is a there is currently a readme and then like feature notebook and model notebook and then validations plus score, right? And then  there are two different separate notebooks in the when we when we deploy this in an MLA environment. But we we decided that the way we will make users run these notebooks is through an orchestration job. Like a like a job or a A just wanted to know the plan.'}]"
     ]
    }
   ],
   "source": [
    "summarizer(input_text_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c747f1a6-6813-40eb-830c-cbaef56ec72a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "text_summarizer_finetune",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
